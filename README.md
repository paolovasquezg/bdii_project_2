# ğŸ–¼ï¸ IndexaciÃ³n de Descriptores Locales (Multimedia Database)

En esta parte del proyecto se implementÃ³ un sistema de bÃºsqueda para imÃ¡genes usando descriptores locales.  
El objetivo es representar cada imagen como un conjunto de â€œvisual wordsâ€ y luego buscar imÃ¡genes similares usando tÃ©cnicas como TF-IDF, Ã­ndice invertido y KNN.

---

# ğŸ”§ 1. Flujo general del sistema

```mermaid
flowchart LR
    A[Imagen] --> B[ExtracciÃ³n de Descriptores<br>(SIFT / ORB)]
    B --> C[Codebook<br>(Clustering)]
    C --> D[Histogramas TF-IDF]
    D --> E[Ãndice Invertido]
    E --> F[KNN por Similitud de Coseno]
```

---

# ğŸ“Œ 2. ExtracciÃ³n de CaracterÃ­sticas

Para cada imagen se extraen **descriptores locales** (como SIFT u ORB).  
Cada descriptor representa una pequeÃ±a parte de la imagen en forma de vector numÃ©rico.

- Una imagen â†’ muchos descriptores.  
- Todos se guardan para construir el codebook.

---

# ğŸ“š 3. ConstrucciÃ³n del Codebook

Se agrupan todos los descriptores usando un algoritmo propio de K-Means (sin sklearn).  
Cada cluster representa una **visual word**.

```mermaid
flowchart LR
    A[Descriptores de todas las imÃ¡genes] --> B[AgrupaciÃ³n en k clusters]
    B --> C[Centroides]
    C --> D[Codebook<br>k visual words]
```

Cada imagen luego se convierte en un histograma que indica cuÃ¡ntas visual words contiene.

---

# ğŸ“Š 4. Histogramas + TF-IDF

Cada imagen se convierte en un **histograma** de visual words.  
DespuÃ©s se aplica **TF-IDF** para ponderar visual words importantes.

```mermaid
flowchart TD
    A[Descriptores de una imagen] --> B[AsignaciÃ³n al cluster mÃ¡s cercano]
    B --> C[Histograma de frecuencias]
    C --> D[TF-IDF<br>peso por importancia global]
```

---

# ğŸ—„ï¸ 5. Ãndice Invertido

Se construye un Ã­ndice donde **cada visual word apunta a las imÃ¡genes donde aparece**, igual que en motores de bÃºsqueda de texto.

```mermaid
flowchart LR
    A[Visual Word ID] --> B[Lista de imÃ¡genes donde aparece]
    B --> C[Peso TF-IDF por imagen]
```

Ejemplo visual:

```mermaid
graph TD
    W0["Word 0"] --> I1["img_01 (0.12)"]
    W0 --> I3["img_03 (0.15)"]
    W1["Word 1"] --> I2["img_02 (0.08)"]
    W1 --> I3
    W2["Word 2"] --> I1
```

---

# ğŸ” 6. BÃºsqueda KNN (Similitud de Coseno)

Para buscar imÃ¡genes similares:

1. Se extraen descriptores de la imagen de consulta.  
2. Se genera su histograma TF-IDF.  
3. Se compara contra el Ã­ndice invertido.  
4. Se calcula la similitud de coseno.  
5. Se usan heaps para obtener los K mÃ¡s parecidos.

```mermaid
flowchart TD
    A[Imagen de consulta] --> B[Histograma TF-IDF]
    B --> C[ComparaciÃ³n contra Ã­ndice invertido]
    C --> D[Similitud de coseno]
    D --> E[Heap con K resultados]
    E --> F[Top K imÃ¡genes similares]
```

---

# ğŸ“ˆ 7. GrÃ¡ficos incluidos

## ğŸ”¹ Costo relativo por etapa

```mermaid
pie showData
    title Costo relativo por etapa
    "ExtracciÃ³n de descriptores" : 45
    "ConstrucciÃ³n del codebook" : 30
    "TF-IDF" : 10
    "Ãndice invertido" : 5
    "BÃºsqueda KNN" : 10
```

---

## ğŸ”¹ ComparaciÃ³n: BÃºsqueda Secuencial vs Ãndice Invertido

```mermaid
bar
    title ComparaciÃ³n de tiempos
    xaxis Imagen
    yaxis ms
    "Secuencial" 120 110 130 125
    "Ãndice invertido" 15 12 18 14
```

---

## ğŸ”¹ DistribuciÃ³n de visual words

```mermaid
pie showData
    title DistribuciÃ³n de palabras visuales
    "Word 0" : 12
    "Word 1" : 8
    "Word 2" : 20
    "Word 3" : 14
    "Word 4" : 10
    "Otros" : 36
```

---

# ğŸ“¦ 8. Archivos generados

Los datos del sistema se guardan en archivos binarios para evitar recalcular todo:

- `codebook.bin` â†’ centroides  
- `df.bin` â†’ Document Frequency  
- `idf.bin` â†’ IDF  
- `histograms.bin` â†’ histogramas TF-IDF  
- `inverted_index.bin` â†’ Ã­ndice invertido  

Esto hace que el sistema pueda cargarse rÃ¡pido sin recomputar los descriptores.

---

# âœ”ï¸ Resumen general

- Se extraen descriptores locales por imagen.  
- Se construye un codebook (visual words).  
- Se crean histogramas TF-IDF para cada imagen.  
- Se construye un Ã­ndice invertido.  
- Se implementa KNN con similitud de coseno.  
- Todo se guarda en binarios para reutilizarlo.

